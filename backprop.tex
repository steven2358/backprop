\documentclass{article}
\usepackage{amsmath,tikz,url}

\begin{document}

\title{Derivation of the Backpropagation Algorithm}
\author{Steven Van Vaerenbergh}
\date{August 20, 2014}

\maketitle

\begin{abstract}
The purpose of this document is to provide a full derivation of the basic backpropagation algorithm, used for training the weights of a neural network.
\end{abstract}

\section{Introduction}

The basic principles of feedforward artificial neural networks are assumed to be known.

\subsection{Diagram}

%Consider a neural network with several hidden layers, as shown in Fig.~\ref{fig:nn}.

\begin{figure}[ht]
\def\layersep{2.5cm}
\centering
% Inspired by http://www.texample.net/tikz/examples/neural-network/
\begin{tikzpicture}[shorten >=1pt,->,draw=black, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[draw,circle,minimum size=17pt,inner sep=0pt]
    \tikzstyle{blank}=[circle,minimum size=17pt,inner sep=0pt]
    \tikzstyle{annot} = [text width=4em, text centered]

    % input layer
    \foreach \name / \y in {1,...,3}
        \node[neuron] (I-\name) at (0,-\y) {$x_\name$};
    \node[neuron] (I-4) at (0,-4) {$+1$};

    % hidden layer 1
    \foreach \name / \y in {1,...,3}
        \path[yshift=0cm]
            node[neuron] (H1-\name) at (\layersep,-\y cm) {};
    \path[yshift=0cm] node[neuron] (H1-4) at (\layersep,-4 cm) {+1};

    % hidden layer 2
    \foreach \name / \y in {1,...,2}
        \path[yshift=-0.5cm]
            node[neuron] (H2-\name) at (2*\layersep,-\y cm) {};
    \path[yshift=0cm] node[neuron] (H2-3) at (2*\layersep,-3.5 cm) {+1};

    % output layer
    \foreach \name / \y in {1,...,2}
        \path[yshift=-1cm]
            node[neuron] (O-\name) at (3*\layersep,-\y cm) {};

    % output layer for labels
    \foreach \name / \y in {1,...,2}
        \path[yshift=-1cm]
            node[blank] (O2-\name) at (3.5*\layersep,-\y cm) {};

%    % output layer
%    \node[neuron,pin={[pin edge={->}]right:output}, right of=O-1] (O) {};

    % connect input to hidden layer 1
    \foreach \source in {1,...,4}
        \foreach \dest in {1,...,3}
            \path (I-\source) edge (H1-\dest);

    % connect hidden layer 1 to hidden layer 2
    \foreach \source in {1,...,4}
        \foreach \dest in {1,...,2}
            \path (H1-\source) edge (H2-\dest);

    % connect hidden layer 2 to output
    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,2}
            \path (H2-\source) edge (O-\dest);

    % connect hidden layer 2 to output
    \foreach \source in {1,...,2}
        \path (O-\source) edge (O2-\source);
        
    \node[annot,right of=O-1][xshift=-1.2cm,yshift=-0.5cm] {$h_{W,b}(x)$};

    % annotate the layers
    \node[annot,above of=H1-1, node distance=1cm] (hl) {hidden layer 1};
    \node[annot,left of=hl] {input layer};
    \node[annot,right of=hl] (h2) {hidden layer 2};
    \node[annot,right of=h2] {output layer};
\end{tikzpicture}
\caption{Diagram of a neural network with several hidden layers.}
\label{fig:nn}
\end{figure}

\subsection{Notation}

In this document we denote by $W_{ij}^{(l)}$ the weight from the $j$-th neuron in the $l$-th layer to the $i$-th neuron of the $l+1$-th layer. This notation is adapted from  \cite{ufldl} and it is often used in the literature. As such, one layer of the network consists of a weighted sum of inputs $z_i^{(l)}$, an activation function, an activation term $a_i^{(l)}$, and a set of connection weights $W_{ij}^{(l)}$, in this order.

\begin{table}[ht]
\begin{tabular}{l|p{10cm}}
$x_i$ & The $i$-th input to the network \\
$f(\cdot)$ & The activation function of the neurons \\
$a_i^{(l)}$ & The output (or ``activation'') of unit $i$ in layer $l$. Note: $x_i = a_i^{(1)}$. \\
$W_{ij}^{(l)}$ & The weight from the $j$-th neuron in the $l$-th layer to the $i$-th \newline neuron of the $l+1$-th layer \\
$z_i^{(l)}$ & The total weighted sum of inputs to unit $i$ in layer $l$ \\
$n_l$ & The number of layers in the neural network (including input and output layers) \\
$s_l$ & The number of neurons in the $l$-th layer \\
$\alpha$ & The learning rate
\end{tabular}
\end{table}

\begin{figure}[ht]
\def\layersep{2.5cm}
\centering
\begin{tikzpicture}[shorten >=1pt,->,draw=black, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[draw,circle,minimum size=17pt,inner sep=0pt]
    \tikzstyle{blank}=[circle,minimum size=17pt,inner sep=0pt]
    \tikzstyle{annot} = [text width=4em, text centered]

    % input
    \foreach \name / \y in {1,...,3}
        \node[blank] (a1-\name) at (0,-\y) {$a_\name^{(2)}$};

    % plus and bias
    \path node[neuron] (sum) at (\layersep,-2 cm) {$\Sigma$};
	\node[blank] (bias) at (\layersep,-3.5 cm) {$b_5^{(2)}$};
	
    % activation function
    \node[neuron] (af) at (1.7*\layersep,-2 cm) {$f(\cdot)$};
    
    % activation value
    \node[blank] (a2) at (2.3*\layersep,-2cm) {$a_5^{(3)}$};
            
    % connect
    \path (a1-1) edge node [above] {$W_{51}^{(2)}$} (sum);
    \path (a1-2) edge node [blank,fill=white] {$W_{52}^{(2)}$} (sum);
    \path (a1-3) edge node [below] {$W_{53}^{(2)}$} (sum);
    \path (bias) edge (sum);
    
    \path (sum) edge node [above] {$z_{5}^{(3)}$} (af);
    \path (af) edge (a2);

\end{tikzpicture}
\caption{Detailed diagram of the used notation. As an example, this diagram focuses on the fifth neuron of the third layer; it starts at the activations of the previous layer that affect this neuron, and it ends with its own activation.}
\label{fig:neuron}
\end{figure}

We write $n_l$ to denote the number of layers in the neural network (including input and output layer), and $s_l$ is the number of neurons in the $l$-th layer. The output of unit $i$ in layer $l$ is is denoted as $a_i^{(l)}$. For the input layer, this means the $i$-th input can be written as $x_i = a_i^{(1)}$. Finally, we denote the total weighted sum of inputs to unit $i$ in layer $l$, including the bias term, as
\begin{equation}
z_i^{(l+1)} = \sum_{j=1}^{s_{l}} W^{(l)}_{ij} a_j^{(l)} + b^{(l)}_i,
\label{eq:weightedsum}
\end{equation}
so that $a^{(l)}_i = f(z^{(l)}_i)$.

By adopting a matrix representation we obtain the following compact relations
%z^{(2)} & = W^{(1)} x + b^{(1)} \\
\begin{align}
z^{(l+1)} & = W^{(l)} a^{(l)} + b^{(l)} \\
a^{(l)} & = f(z^{(l)}).
\end{align}
The output of the last layer is denoted $h_{W,b}(x)$,
\begin{equation}
h_{W,b}(x) = a^{(n_l)} = f(z^{(n_l)}).
\end{equation}


\section{Derivation of the backpropagation algorithm}

Suppose we have a fixed training set $(x^{(1)}, y^{(1)}), \dots, (x^{(m)}, y^{(m)})$ of $m$ training examples. We can train our neural network using batch gradient descent. Define the cost function with respect to a single example $(x,y)$ to be
\begin{equation}
J(W,b;x,y) = \frac{1}{2} \|h_{W,b}(x)-y\|^2.
\label{eq:costsingle}
\end{equation}
%
Define the overall cost function to be
\begin{equation}
J(W,b) = \left[ \frac{1}{m} \sum_{k=1}^m J(W,b;x^{(k)},y^{(k)}) \right] + \frac{\lambda}{2} \sum_{l=1}^{n_l-1} \; \sum_{i=1}^{s_l} \; \sum_{j=1}^{s_{(l+1)}} \left( W^{(l)}_{ji} \right)^2.
\end{equation}

One iteration of gradient descent updates the paramzters $W$, $b$ as follows
\begin{align}
W_{ij}^{(l)} & = W_{ij}^{(l)} - \alpha \frac{\partial}{\partial W_{ij}^{(l)}} J(W,b) \\
b_{i}^{(l)} & = b_{i}^{(l)} - \alpha \frac{\partial}{\partial b_{i}^{(l)}} J(W,b),
\end{align}
where $\alpha$ is the learning rate. The key step is computing the partial derivatives above.

Backpropagation is used to compute $\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b; x, y)$ and $\frac{\partial}{\partial b_{i}^{(l)}} J(W,b; x, y)$, the partial derivatives of the cost function $J(W,b;x,y)$ defined with respect to a single example $(x,y)$. Once we can compute these, we see that the derivative of the overall cost function $J(W,b)$ can be computed as
\begin{align}
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b) & =
\left[ \frac{1}{m} \sum_{k=1}^m \frac{\partial}{\partial W_{ij}^{(l)}} J(W,b; x^{(k)}, y^{(k)}) \right] + \lambda W_{ij}^{(l)} \\
\frac{\partial}{\partial b_{i}^{(l)}} J(W,b) & =
\frac{1}{m}\sum_{k=1}^m \frac{\partial}{\partial b_{i}^{(l)}} J(W,b; x^{(k)}, y^{(i)})
\end{align}

The derivation of the backpropagation algorithm is based on the chain rule. We wish to measure the gradient of the cost function with respect to each of the weights and biases,
%For the derivative with respect to $W_{ij}^{(l)}$, we can write
\begin{align}
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b; x, y) & = \frac{\partial J}{\partial z_i^{(l+1)}} \frac{\partial z_i^{(l+1)}}{\partial W_{ij}^{(l)}} \label{eq:chainw}\\
\frac{\partial}{\partial b_i^{(l)}} J(W,b; x, y) & = \frac{\partial J}{\partial z_i^{(l+1)}} \label{eq:chainb}
\end{align}
% & = \delta_i^{(l)} \frac{\partial z_i^{(l)}}{\partial W_{ij}^{(l)}},
where we have abbreviated $J(W,b; x, y)$ to $J$. In Eq.~\eqref{eq:chainb} we have taken into account that $\partial z_i^{(l+1)} / \partial b_i^{(l)} = 1$, which is obtained from Eq.~\eqref{eq:weightedsum}. In Eq.~\eqref{eq:chainw}, we find that the second factor on the right can be obtained as
\begin{equation}
\frac{\partial z_i^{(l+1)}}{\partial W_{ij}^{(l)}} = a_i^{(l)}
\end{equation}
The first factor in the right-hand side of Eq.~\eqref{eq:chainw} measures to what extent node $i$ was ``responsible'' for any errors in the network's output, and it is denoted as the error term,
\begin{equation}
\delta_i^{(l)} = \frac{\partial J}{\partial z_i^{(l)}}.
\end{equation}
The ``delta term'' is the central piece of the backpropagation algorithm. By applying the chain rule again, it is decomposed as
\begin{equation}
\delta_i^{(l)} = \frac{\partial J}{\partial a_i^{(l)}} \frac{\partial a_i^{(l)}}{\partial z_i^{(l)}} = \frac{\partial J}{\partial a_i^{(l)}} f'(z_i^{(l)}).
\label{eq:deltachain}
\end{equation}
In order to analyze the first factor in the expansion of Eq.~\eqref{eq:deltachain}, we start by expanding the cost function for a single example, from Eq.~\eqref{eq:costsingle},
\begin{equation}
J(W,b;x,y) = \frac{1}{2} \sum_{i=1}^{s_{n_l}} (a_i^{(l)}-y_i)^2
\end{equation}
%\begin{equation}
%J(W,b;x,y) = \frac{1}{2} \left\| \begin{bmatrix}
%a_1^{(n_l)}\\ \vdots \\ a_{s_{n_l}}^{(n_l)}
%\end{bmatrix} - \begin{bmatrix} y_1\\ \vdots\\ y_{s_{n_l}} \end{bmatrix} \right\|^2 = \frac{1}{2} \left\| \begin{bmatrix}
%f(z_1^{(n_l)})\\ \vdots \\ f(z_{s_{n_l}}^{(n_l)})
%\end{bmatrix} - \begin{bmatrix} y_1\\ \vdots\\ y_{s_{n_l}} \end{bmatrix} \right\|^2.
%\end{equation}
For the last layer, $l=n_l$, we obtain
\begin{equation}
\frac{\partial J}{\partial a_i^{(n_l)}} = a_i^{(n_l)} - y_i.
\end{equation}
For any of the previous layers, $l<n_l$, the derivation requires some additional calculation. First observe that the output of node $i$ in this layer, $a_i^{(l)}$, affects the weighted inputs $z_j^{(l+1)}$ of each the nodes $j$ in the following layer, by virtue of Eq.~\eqref{eq:weightedsum}. Applying the chain rule to the first factor in the expansion of Eq.~\eqref{eq:deltachain} now yields
\begin{equation}
\frac{\partial J}{\partial a_i^{(l)}} = \sum_{j=1}^{s_{(l+1)}} \frac{\partial J}{\partial z_j^{(l+1)}} \frac{\partial z_j^{(l+1)}}{\partial a_i^{(l)}} = \sum_{j=1}^{s_{(l+1)}} \delta_j^{(l+1)} W_{ji}^{(l)}.
\end{equation}
In order to calculate $\partial J / \partial a_i^{(l)}$, we have used the error terms $\delta_j^{(l+1)}$ for the next layer. The backpropagation therefore first calculates the delta terms for the output layer, and then works its way back to calculate the delta terms of each previous layer based on the delta terms of its following layer.


% The derivation of the backpropagation algorithm is based on the chain rule. For the derivative with respect to $W_{ij}^{(l)}$, we can write
% \begin{equation}
% \frac{\partial}{\partial W_{ij}^{(l)}} J(W,b; x, y) = \frac{\partial J}{\partial a_i^{(l)}} \frac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \frac{\partial z_i^{(l)}}{\partial W_{ij}^{(l)}},
% \end{equation}
% where we have abbreviated $J(W,b; x, y)$ to $J$.



\begin{thebibliography}{1}

\bibitem{ufldl} Andrew Ng, {\em UFLDL Tutorial}, \url{http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial}, 2011.

\end{thebibliography}

\end{document}
